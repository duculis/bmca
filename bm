#!/usr/bin/env python
import os
import re
from subprocess import call, TimeoutExpired
from clize import run
from datetime import datetime
import sqlite3
from bs4 import BeautifulSoup
import requests
from tabulate import tabulate
import pandas as pd
from random import shuffle

pd.set_option("display.max_colwidth", -1)
db_path = os.path.join(os.getenv("HOME"), ".bm.db")
cache_path = os.path.join(os.getenv("HOME"), ".bm_cache")
pdf_viewer = "atril"
chrome = "chromium-browser"


def a(url, *, desc="", tags=""):
    """
    Add a url to the bookmark database

    desc : description (optional)
    tags : list of tags separated by space (optional)

    Examples:

    bm a "https://www.sqlite.org/docs.html"

    bm a "https://www.sqlite.org/docs.html" --desc='sqlite documentation' 
    
    bm a "https://www.sqlite.org/docs.html" --tags='database documentation reference' # 3 tags, "database", "documentation", and "reference"

    """
    db = sqlite3.connect(db_path)
    db.execute(
        "CREATE TABLE IF NOT EXISTS urls(id INTEGER primary key,"
        "time TIMESTAMP, url TEXT UNIQUE, title TEXT, "
        "description TEXT, tags TEXT, views INT)"
    )
    db.commit()
    time = datetime.now()
    title = _get_title(url)
    title = title.replace("'", "")
    print("Adding url : {}, title : {}...".format(url, title))
    query = (
        "INSERT INTO urls VALUES("
        "NULL, '{time}', '{url}', '{title}', "
        "'{desc}', '{tags}', {views})".format(
            time=time, url=url, title=title, desc=desc, tags=tags, views=0
        )
    )
    try:
        db.execute(query)
        db.commit()
    except sqlite3.IntegrityError:
        print('Failed, url "{}" Already exists in the database.'.format(url))


def ls(
    keywords="",
    *,
    id="",
    sort_by="time",
    order="desc",
    nb=-1,
    trim=True,
    random=False,
    open_in_browser=False,
    open_pdf=False,
    open_all=False,
    query_mode="and",
    fields="all",
):
    """
    display the list of urls in the bookmark

    keywords : str
        list of keywords separated by space
        to search for in the urls or titles or tags

    id : int
        specic id of a url to display

    sort_by : str
        sort the urls by 'views' or 'id' or 'url' or 'time'

    order: str
        asc or desc

    nb : int
        number of urls to display

    trim : bool
        whether to trim long urls or titles

    random: bool
        whether the result is in random order

    open-in-browser : true or false
        whether to open the first url (or all of them if "open-all") in the diplayed list
        in the browser

    open-pdf : true or false
        whether to open the cached pdfs of the pages (alternative to 'open-in-browser')

    open-all : true or false
        whether to open all urls in the displayed list
        (works along with 'open-in-browser' or 'open-pdf')

    query_mode : str
        either "and" or "or"
        specify whether ALL keywords ("and") of the query must be in the
        url or any of them ("or").

    fields : str
        specify the fields where to look for the keywords.
        it should be either "all" or  "title", "desc", "tags", "url"
        separated by commas, e.g., fields could be "title,desc".

    Examples:

    bm ls # display all

    bm ls 'health insurance' #look for "health" and "insurance" (both should be present) in urls, titles, tags and description.
    
    bm ls 'health insurance' --query-mode=or # look for either "health" OR "insurance" (any should be present) in urls, titles, tags, and description)

    bm ls 'health insurance' --nb=5 # show 5 first (sorted by time)

    bm ls 'health insurance' --nb=10 --sort-by=views # show 10 first (sorted by views desc)

    bm ls 'health insurance' --nb=10 --sort-by=views --order=asc #  show 10 first (sorted by views asc)

    bm ls --random #show urls in random order
    
    bm ls --random --nb=5 # show 5 randomly chosen urls

    bm ls 'health insurance' --random --nb=5 # show 5 randomly chosen urls which contain they keywords "health" and "insurance"

    bm ls --id=142 #look for a specfic url (each url has an id)

    bm ls --tags='video' #look for all urls tagged as "video"

    bm ls --trim=false  #do not trim the urls (trimming is used to constrain the urls to be in one line)

    bm ls --id=142 --open-in-browser #open a specific url in the browser
    
    bm ls 'machine learning' --open-in-browser --open-all # open all machine learning related urls in browser

    bm ls 'machine learning' --open-pdf --open-all # open all machine learning  related urls in pdf (cached version)

    """
    order = order.upper()
    assert order in ("DESC", "ASC")
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    max_length = (
        50
        if trim and not open_in_browser and not open_pdf and not open_all
        else float("inf")
    )
    db = sqlite3.connect(db_path)
    where_request = _build_where_request(
        keywords.split(" "), query_mode=query_mode, fields=fields
    )
    # update nb of views only when we search for something
    if keywords != "":
        query = "UPDATE urls SET views = views + 1 WHERE {where_request}".format(
            where_request=where_request
        )
        db.execute(query)
        db.commit()

    # Prepare the search query depending on the situation
    if id:
        # ID given, show exactly one url
        query = "SELECT id, url, title, views, time FROM urls " "WHERE id={}".format(id)
    elif keywords == "":
        # no keyword, return all urls
        query = "SELECT id, url, title, views, time FROM urls ORDER BY {sort_by} {order}".format(
            sort_by=sort_by, order=order
        )
    else:
        # keywords given, look for urls which match the keywords
        query = (
            "SELECT id, url, title, views, time FROM urls WHERE "
            "{where_request} ORDER BY {sort_by} {order}".format(
                where_request=where_request, sort_by=sort_by, order=order
            )
        )

    def _format_row(row):
        id, url, title, views, time = row
        url = _format(url, max_length=max_length)
        title = _format(title, max_length=max_length)
        return id, url, title, views

    # Get the results from DB
    rows = db.execute(query)
    rows = map(_format_row, rows)
    rows = list(rows)
    if random:
        shuffle(rows)
    if nb > 0:
        rows = rows[0:nb]
    print(tabulate(rows, headers=["id", "url", "title", "views"]))

    if open_in_browser or open_all or open_pdf:
        if not open_all:
            rows = rows[0:1]
        if open_pdf:
            for _, url, title, _ in rows:
                pdf = _get_cached(title, url)
                _view_pdf(pdf)
        else:
            for _, url, _, _ in rows:
                _view_on_browser(url)


def _format(s, max_length=50):
    s = s.replace("\n", "")
    if len(s) > max_length:
        s = s[0:max_length] + "(...)"
    return s


def rm(keywords="", *, id="", query_mode="and", fields="all"):
    """
    remove a list of urls or one url
    from the bookmarks database

    keywords : str
        list of keywords to search for separated by space
    id : int
        id of a specific url to remove (optional)

    for "query_mode" and "fields", check the definition of "ls"

    Examples:

    bm rm --id=143 #remove a specific url

    bm rm 'health insurance' #remove all urls which match the keywords "health" and "insurance"

    """
    assert (keywords and id == "") or (int(id) >= 0 and not keywords)
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    if id:
        query = "SELECT id, url, title FROM urls WHERE id={}".format(id)
    else:
        where_request = _build_where_request(
            keywords.split(" "), query_mode=query_mode, fields=fields
        )
        query = "SELECT id, url, title FROM urls WHERE {}".format(where_request)
    db = sqlite3.connect(db_path)
    for id, url, title in db.execute(query):
        response = ""
        while response not in ("y", "n"):
            print("Delete {} ? (y/n)".format(url), end="")
            response = input()
        if response == "y":
            db.execute("DELETE FROM urls WHERE id={}".format(id))
    db.commit()


def export_csv(out):
    """
    export the database into csv

    out : str
        filename where to write the csv

    Examples:

    bm export out.csv
    """
    df = _export_db_to_dataframe()
    df.to_csv(out, index=False)


def export_html(out, *, order_by="views"):
    """
    export the database into HTML

    Examples:

    bm export out.html
    """
    df = _export_db_to_dataframe(order_by=order_by)
    df["url"] = df["url"].apply(_url_to_html_link)
    df["time"] = pd.to_datetime(df["time"]).apply(_datetime_to_date)
    df = df.sort_values(by="time", ascending=False)
    content = df.to_html(index=False, escape=False, col_space=100)
    html = """
    <!doctype html>
    <html>
    <head>
    <meta charset="UTF-8">
    </head>
    {}
    </html>
    """.format(
        content
    )
    with open(out, "w") as fd:
        fd.write(html)


def _export_db_to_dataframe(order_by="views"):
    q = "SELECT id, time, url, title, tags, views FROM urls ORDER BY {}".format(
        order_by
    )
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    rows = []
    for row in c.execute(q):
        rows.append(row)
    cols = ["id", "time", "url", "title", "tags", "views"]
    df = pd.DataFrame(rows, columns=cols)
    return df


def _datetime_to_date(time):
    return time.date()


def _url_to_html_link(url):
    return '<a href="{url}">{url}</a>'.format(url=url)


def tags():
    """
    list of available tags
    """
    query = "SELECT tags FROM urls"
    db = sqlite3.connect(db_path)
    tags = [tag for tag_str, in db.execute(query) for tag in tag_str.split(" ")]
    tags = sorted(list(set(tags)))
    for t in tags:
        print(t)


def tag(keywords="", *, id="", query_mode="and", fields="all"):
    """
    add/override tags of urls interactively

    keywords : str
        keywords to search for separated by space
    
    id : int
        id of a specific url to tag

    for "query_mode" and "fields", check the definition of "ls"
    
    Examples:

    bm tag #look for urls without tags to tag them

    bm tag 'health' # override tags of the urls matching "health"

    bm tag --id=141 # override tags of a specific url
    """
    assert (keywords and id == "") or (int(id) >= 0 and not keywords)
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    if id:
        query = "SELECT id, url, title, tags FROM urls WHERE id={}".format(id)
    else:
        where_request = _build_where_request(
            keywords.split(" "), query_mode=query_mode, fields=fields
        )
        if keywords == "":
            query = "SELECT id, url, title, tags FROM urls WHERE tags=''"
        else:
            query = "SELECT id, url, title, tags FROM urls WHERE {}".format(where_request)
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    for id_, url, title, tags in c.execute(query):
        print(f"'{url}, '{title}'")
        if not tags:
            print("No tags with this url")
            print("New tags ? ", end="")
        else:
            print(f"Current tags: {tags}")
            print("Override tags ?", end="")
        tag = input()
        if tag != "":
            conn.execute("UPDATE urls SET tags='{}' WHERE id={}".format(tag, id_))
            conn.commit()


def reset_views():
    """
    reset the number of views of ALL urls to 0
    """
    conn = sqlite3.connect(db_path)
    q = "UPDATE urls SET views = 0"
    c = conn.cursor()
    c.execute(q)
    conn.commit()


def _get_title(url):
    """
    get the title of a web page
    """
    soup = BeautifulSoup(requests.get(url).content, "lxml")
    if soup.title is None:
        return ""
    return soup.title.text


def _build_where_request(keywords, query_mode="and", fields="all"):
    """
    Get the where part of sql query title or url should
    match any of the keywords

    keywords : list of str
    """
    if fields == "all":
        fields = "title,url,description,tags"
    req = [field + " LIKE '%{q}%'" for field in fields.split(",")]
    req = " OR ".join(req)
    req = "(" + req + ")"
    if query_mode == "and":
        method = " AND "
    elif query_mode == "or":
        method = " OR "
    else:
        raise ValueError(method)
    search_req = method.join([req.format(q=k) for k in keywords])
    return search_req


def recommend(
    keywords="", *, nb=5, id="", open_in_browser=False, open_all=False, open_pdf=False
):
    """
    recommend links based either on keywords or on the active window titles
    in XORG. The recommendation is based on the TF-IDF similarity between keywords 
    (or titles of active windows) and the url/title of url. It's an alternative to "ls" 
    which looks for hard matching between keywords and urls.
    
    keywords : str
        keywords separated by space, e.g., "deep learning machine"

    nb : int
        number of urls to display

    for "open-in-browser", "open-pdf", "open-all", check "ls".

    Examples:

    bm recommend #here it just uses active window titles to look for matches
    
    bm recommend "deep learning" #here it uses the keywords "deep" and "learning" to look for matches

    bm recommend "deep learning" --nb=5 --open-in-browser

    bm recommend "deep learning" --nb=5 --open-pdf

    """
    from functools import partial
    from sklearn.feature_extraction.text import TfidfVectorizer

    df = _export_db_to_dataframe()
    if keywords:
        titles = keywords.split(" ")
    else:
        titles = _get_active_window_titles()
    df["url_title"] = df.apply(lambda d: "{} {}".format(d["url"], d["title"]), axis=1)
    tfidf = TfidfVectorizer().fit(df["url_title"].values.tolist())
    for i, title in enumerate(titles):
        df["sim_{}".format(i)] = df["title"].apply(
            partial(_similarity_score, tfidf, title)
        )
    df["sim"] = df.filter(like="sim_", axis=1).max(axis=1)
    df = df.sort_values(by="sim", ascending=False)
    df = df.iloc[0:nb]
    df["url_full"] = df["url"].copy()
    df["url"] = df["url"].apply(partial(_format, max_length=100))
    cols = df[["id", "url", "sim"]].to_dict(orient="list")
    print(tabulate(cols, headers="keys"))
    if open_in_browser or open_all or open_pdf:
        if not open_all:
            df = df.iloc[0:1]
        if open_pdf:
            for _, row in df.iterrows():
                pdf = _get_cached(row.title, row.url)
                _view_pdf(pdf)
        else:
            for _, row in df.iterrows():
                _view_on_browser(row.url)


def _get_active_window_titles():
    # get active window titles in XORG
    from subprocess import check_output

    output = check_output("wmctrl -l", shell=True).decode()
    lines = output.split("\n")
    titles = []
    for line in lines:
        line = line.strip()
        if len(line) == 0:
            continue
        _, state, _, *rest = re.split("[ \t]+", line)
        title = " ".join(rest)
        if state != "-1":
            if " - " in title:
                title, *rest = title.split(" - ")
                titles.append(title)
    return titles


def _similarity_score(tfidf, a, b):
    import numpy as np

    x = tfidf.transform([a, b])
    x = np.array(x.todense())
    av = x[0]
    bv = x[1]
    eps = 1e-10
    return np.dot(av, bv) / ((np.linalg.norm(av) + eps) * (np.linalg.norm(bv) + eps))


def update_cache():
    if not os.path.exists(cache_path):
        os.makedirs(cache_path)
    _clean_cache_folder()
    df = _export_db_to_dataframe()
    for _, row in df.iterrows():
        _get_cached(row.title, row.url)


def _clean_cache_folder():
    # remove all cached files which do not correspond
    # to an entry in the DB
    df = _export_db_to_dataframe()
    cache_filenames = []
    for _, row in df.iterrows():
        filename = _get_cache_path(row.title, row.url)
        cache_filenames.append(filename)
    cache_filenames = set(cache_filenames)
    for filename in os.listdir(cache_path):
        filename_full_path = os.path.join(cache_path, filename)
        if filename_full_path not in cache_filenames:
            print(f"Removing {filename_full_path}...")
            os.remove(filename_full_path)


def _get_cached(title, url, reset=False):
    cache_path = _get_cache_path(title, url)
    if os.path.exists(cache_path) and not reset:
        return cache_path
    else:
        _cache(url, dest=cache_path)
        return cache_path


def _get_cache_path(title, url):
    # replace special characters with underscore
    slug = re.sub(r"[/!@#$%^&*(),.?\":{}|<>+'\[\]]", "_", url)
    slug = slug.lower()
    filename = slug[0:100] + ".pdf"
    filename_full_path = os.path.join(cache_path, filename)
    return filename_full_path


def _cache(url, dest):
    print(f"Caching {url} to {dest}")
    if "arxiv" in url:
        url = url.replace("/abs/", "/pdf/") + ".pdf"
        cmd = f"curl --output '{dest}' '{url}'"
    elif url.endswith(".pdf"):
        cmd = f"curl --output '{dest}' '{url}'"
    else:
        cmd = f"{chrome} --headless  --disable-gpu --print-to-pdf='{dest}' '{url}'"
    try:
        call(cmd, shell=True, timeout=60)
    except TimeoutExpired:
        pass
    finally:
        if not os.path.exists(dest):
            with open(dest, "w") as _:
                pass



def _tokenize(s):
    return re.split(r"[ \t\./-]+", s)


def _view_on_browser(url):
    import webbrowser

    webbrowser.open(url)


def _view_pdf(pdf):
    call(f"{pdf_viewer} {pdf}", shell=True)


if __name__ == "__main__":
    run(
        [
            a,
            ls,
            rm,
            export_csv,
            export_html,
            tag,
            reset_views,
            tags,
            recommend,
            update_cache,
        ]
    )
