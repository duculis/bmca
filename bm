#!/usr/bin/env python
import os
import re
from subprocess import call, TimeoutExpired
from clize import run
from datetime import datetime
import sqlite3
from bs4 import BeautifulSoup
import requests
from tabulate import tabulate
import pandas as pd
from random import randint

pd.set_option("display.max_colwidth", -1)
db_path = os.path.join(os.getenv("HOME"), ".bm.db")
cache_path = os.path.join(os.getenv("HOME"), ".bm_cache")
pdf_viewer = "atril"
chrome = "chromium-browser"


def a(url, *, desc="", tags=""):
    """
    Add a url to the bookmark database

    desc : description (optional)
    tags : list of tags separated by space (optional)

    Examples:

    bm a "http://github.com/mehdidc/bm"

    bm a "http://github.com/mehdidc/bm" --tags='bm bookmark'

    bm a "http://github/com/mehdidc/bm" --desc='bookmark in command line'
    """
    db = sqlite3.connect(db_path)
    db.execute(
        "CREATE TABLE IF NOT EXISTS urls(id INTEGER primary key,"
        "time TIMESTAMP, url TEXT UNIQUE, title TEXT, "
        "description TEXT, tags TEXT, views INT)"
    )
    db.commit()
    time = datetime.now()
    title = _get_title(url)
    title = title.replace("'", "")
    print("Adding url : {}, title : {}...".format(url, title))
    query = (
        "INSERT INTO urls VALUES("
        "NULL, '{time}', '{url}', '{title}', "
        "'{desc}', '{tags}', {views})".format(
            time=time, url=url, title=title, desc=desc, tags=tags, views=0
        )
    )
    try:
        db.execute(query)
        db.commit()
    except sqlite3.IntegrityError:
        print('Failed, url "{}" Already exists in the database.'.format(url))


def ls(
    keywords="",
    *,
    id="",
    sort_by="time",
    order="desc",
    nb=-1,
    trim=True,
    random=False,
    open=False,
    open_pdf=False,
    open_all=False,
    query_mode="and",
    fields="all"
):
    """
    display the list of urls in the bookmark

    keywords : str
        list of keywords separated by space
        to search for in the urls or titles or tags
    id : int
        specic id of a url to display
    sort_by : str
        sort the urls by 'views' or 'id' or 'url' or 'time'
    trim : bool
        whether to trim long urls or titles
    random: bool
        whether the result is in random order
    open : true or false
        whether to open the first url in the diplayed list
        in the browser
    open-all : true or false
        whether to open all urls in the displayed list
        in the browser
    nb : int
        number of rows to display

    Examples:

    bm ls

    bm ls 'deep learning video'

    bm ls --id=142

    bm ls --tags='video'

    bm ls 'deep learning video' --trim=false

    bm ls --id=142 --open
    """
    order = order.upper()
    assert order in ("DESC", "ASC")
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    max_length = 50 if trim and not random and not open and not open_all else float("inf")
    db = sqlite3.connect(db_path)
    where_request = _build_where_request(
        keywords.split(" "), query_mode=query_mode, fields=fields
    )
    # update nb of views only when we search for something
    if keywords != "":
        query = "UPDATE urls SET views = views + 1 WHERE {where_request}".format(
            where_request=where_request
        )
        db.execute(query)
        db.commit()

    # Prepare the search query depending on the situation
    if id:
        # ID given, show exactly one url
        query = "SELECT id, url, title, views, time FROM urls " "WHERE id={}".format(id)
    elif keywords == "":
        # no keyword, return all urls
        query = "SELECT id, url, title, views, time FROM urls ORDER BY {sort_by} {order}".format(
            sort_by=sort_by, order=order
        )
    else:
        # keywords given, look for urls which match the keywords
        query = (
            "SELECT id, url, title, views, time FROM urls WHERE "
            "{where_request} ORDER BY {sort_by} {order}".format(
                where_request=where_request, sort_by=sort_by, order=order
            )
        )

    def _format_row(row):
        id, url, title, views, time = row
        url = _format(url, max_length=max_length)
        title = _format(title, max_length=max_length)
        return id, url, title, views

    # Get the results from DB
    rows = db.execute(query)
    rows = map(_format_row, rows)
    rows = list(rows)
    if random:
        idx = randint(0, len(rows))
        rows = rows[idx : idx + 1]
    if nb > 0:
        rows = rows[0:nb]
    print(tabulate(rows, headers=["id", "url", "title", "views"]))

    if open or open_all or open_pdf:
        if not open_all:
            rows = rows[0:1]
        if open_pdf:
            for _, url, title, _ in rows:
                pdf = _get_cached(title, url)
                _view_pdf(pdf)
        else:
            import webbrowser
            for _, url, _, _ in rows:
                webbrowser.open(url)


def _format(s, max_length=50):
    s = s.replace("\n", "")
    if len(s) > max_length:
        s = s[0:max_length] + "(...)"
    return s

def _view_pdf(pdf):
    call(f"{pdf_viewer} {pdf}", shell=True)


def rm(keywords="", *, id="", query_mode="and", fields="all"):
    """
    remove a list of urls or one url
    from the bookmarks database

    keywords : str
        list of keywords to search for separated by space
    id : int
        id of a specific url to remove (optional)

    Examples:

    bm rm --id=143

    bm rm 'deep learning video'
    """
    assert (keywords and id == "") or (int(id) >= 0 and not keywords)
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    if id:
        query = "SELECT id, url, title FROM urls WHERE id={}".format(id)
    else:
        where_request = _build_where_request(
            keywords.split(" "), query_mode=query_mode, fields=fields
        )
        query = "SELECT id, url, title FROM urls WHERE {}".format(where_request)
    db = sqlite3.connect(db_path)
    for id, url, title in db.execute(query):
        response = ""
        while response not in ("y", "n"):
            print("Delete {} ? (y/n)".format(url), end="")
            response = input()
        if response == "y":
            db.execute("DELETE FROM urls WHERE id={}".format(id))
    db.commit()


def export_csv(out):
    """
    export the database into csv

    out : str
        filename where to write the csv

    Examples:

    bm export out.csv
    """
    df = _export_db_to_dataframe()
    df.to_csv(out, index=False)


def export_html(out, *, order_by="views"):

    df = _export_db_to_dataframe(order_by=order_by)
    df["url"] = df["url"].apply(url_to_html_link)
    df["time"] = pd.to_datetime(df["time"]).apply(datetime_to_date)
    df = df.sort_values(by="time", ascending=False)
    content = df.to_html(index=False, escape=False, col_space=100)
    html = """
    <!doctype html>
    <html>
    <head>
    <meta charset="UTF-8">
    </head>
    {}
    </html>
    """.format(
        content
    )
    with open(out, "w") as fd:
        fd.write(html)


def _export_db_to_dataframe(order_by="views"):
    q = "SELECT id, time, url, title, tags, views FROM urls ORDER BY {}".format(
        order_by
    )
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    rows = []
    for row in c.execute(q):
        rows.append(row)
    cols = ["id", "time", "url", "title", "tags", "views"]
    df = pd.DataFrame(rows, columns=cols)
    return df


def datetime_to_date(time):
    return time.date()


def url_to_html_link(url):
    return '<a href="{url}">{url}</a>'.format(url=url)


def tags():
    """
    list of available tags
    """
    query = "SELECT tags FROM urls"
    db = sqlite3.connect(db_path)
    tags = [tag for tag_str, in db.execute(query) for tag in tag_str.split(" ")]
    tags = sorted(list(set(tags)))
    for t in tags:
        print(t)


def tag(keywords="", *, id="", query_mode="and", fields="all"):
    """
    override the tags of urls

    q : str
        keywords to search for separated by space
    id : int
        id of a specific url to tag

    Examples:

    bm tag

    bm tag 'deep learning video'

    bm tag --id=141
    """
    assert (keywords and id == "") or (int(id) >= 0 and not keywords)
    assert query_mode in ("and", "or")
    assert fields == "all" or all(
        [f in ("title", "desc", "tags", "url") for f in fields.split(",")]
    )
    if id:
        query = "SELECT id, url, title FROM urls WHERE id={}".format(id)
    else:
        where_request = _build_where_request(
            keywords.split(" "), query_mode=query_mode, fields=fields
        )
        if keywords == "":
            query = "SELECT id, url, title FROM urls WHERE tags=''"
        else:
            query = "SELECT id, url, title FROM urls WHERE {}".format(where_request)
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    for id_, url, title in c.execute(query):
        print(url)
        print(title)
        print("New tag ?", end="")
        tag = input()
        if tag != "":
            conn.execute("UPDATE urls SET tags='{}' WHERE id={}".format(tag, id_))
            conn.commit()


def reset_views():
    conn = sqlite3.connect(db_path)
    q = "UPDATE urls SET views = 0"
    c = conn.cursor()
    c.execute(q)
    conn.commit()


def _get_title(url):
    """
    get the title of a web page
    """
    soup = BeautifulSoup(requests.get(url).content, "lxml")
    if soup.title is None:
        return ""
    return soup.title.text


def _build_where_request(keywords, query_mode="and", fields="all"):
    """
    Get the where part of sql query title or url should
    match any of the keywords

    keywords : list of str
    """
    if fields == "all":
        fields = "title,url,description,tags"
    req = [field + " LIKE '%{q}%'" for field in fields.split(",")]
    req = " OR ".join(req)
    if query_mode == "and":
        method = " AND "
    elif query_mode == "or":
        method = " OR "
    else:
        raise ValueError(method)
    search_req = method.join([req.format(q=k) for k in keywords])
    return search_req


def recommand(*, nb=5, id="", open=False):
    from functools import partial
    from sklearn.feature_extraction.text import TfidfVectorizer

    df = _export_db_to_dataframe()
    titles = _get_active_window_titles()
    df["url_title"] = df.apply(lambda d: "{} {}".format(d["url"], d["title"]), axis=1)
    tfidf = TfidfVectorizer().fit(df["url_title"].values.tolist())
    for i, title in enumerate(titles):
        df["sim_{}".format(i)] = df["title"].apply(partial(similar, tfidf, title))
    df["sim"] = df.filter(like="sim_", axis=1).max(axis=1)
    df = df.sort_values(by="sim", ascending=False)
    df = df.iloc[0:nb]
    df["url_full"] = df["url"].copy()
    df["url"] = df["url"].apply(partial(_format, max_length=100))
    cols = df[["id", "url", "sim"]].to_dict(orient="list")
    print(tabulate(cols, headers="keys"))
    if open:
        import webbrowser

        if id:
            id = int(id)
            url = df.set_index("id").loc[id]["url_full"]
        else:
            url = df.iloc[0]["url_full"]
        webbrowser.open(url)


def _get_active_window_titles():
    from subprocess import check_output

    output = check_output("wmctrl -l", shell=True).decode()
    lines = output.split("\n")
    titles = []
    for line in lines:
        line = line.strip()
        if len(line) == 0:
            continue
        _, state, _, *rest = re.split("[ \t]+", line)
        title = " ".join(rest)
        if state != "-1":
            if " - " in title:
                title, *rest = title.split(" - ")
                titles.append(title)
    return titles


def similar(tfidf, a, b):
    import numpy as np

    x = tfidf.transform([a, b])
    x = np.array(x.todense())
    av = x[0]
    bv = x[1]
    eps = 1e-10
    return np.dot(av, bv) / ((np.linalg.norm(av) + eps) * (np.linalg.norm(bv) + eps))


def update_cache():
    df = _export_db_to_dataframe()
    if not os.path.exists(cache_path):
        os.makedirs(cache_path)
    for _, row in df.iterrows():
        _get_cached(row.title, row.url)


def _get_cached(title, url, reset=False):
    slug = url
    slug = slug.replace(" ", "_")
    slug = slug.replace("/", "_")
    slug = slug.replace(":", "_")
    slug = slug.lower()
    filename = slug + ".pdf"
    filename_full_path = os.path.join(cache_path, filename)
    #if os.path.exists(filename_full_path) and not reset:
    if "arxiv" not in url:
        return filename_full_path
    else:
        return _cache(url, filename_full_path)


def _cache(url, out_path):
    print(f"Caching {url} to {out_path}")
    if "arxiv" in url:
        url = url.replace("/abs/", "/pdf/") + ".pdf"
        cmd = f"curl --output '{out_path}' '{url}'"
    elif url.endswith(".pdf"):
        cmd = f"curl --output '{out_path}' '{url}'"
    else:
        cmd = f"{chrome} --headless  --disable-gpu --print-to-pdf='{out_path}' '{url}'"
    try:
        call(cmd, shell=True, timeout=60)
    except TimeoutExpired:
        return None
    else:
        return out_path


def _tokenize(s):
    return re.split(r"[ \t\./-]+", s)


if __name__ == "__main__":
    run([a, ls, rm, export_csv, export_html, tag, reset_views, tags, recommand, update_cache])
